{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Estate Multimodal Search Engine with Gradio\n",
    "import gradio as gr\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "import os\n",
    "import chromadb\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import random\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating listings...\n",
      "Saved 10 listings to real_estate_listings.txt\n",
      "Generated 10 listings\n",
      "Storing listings...\n",
      "Database initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Load credentials\n",
    "with open(\"credentials.json\", \"r\") as file:\n",
    "    credentials = json.load(file)\n",
    "\n",
    "openai_api_key = credentials[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_BASE\"] = credentials[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# Load models\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Setup ChromaDB\n",
    "CHROMA_PATH = \"chroma_real_estate\"\n",
    "if not os.path.exists(CHROMA_PATH):\n",
    "    os.makedirs(CHROMA_PATH)\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"real_estate_listings\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Using cosine similarity for embeddings\n",
    ")\n",
    "\n",
    "# Initialize LangChain LLM\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "# Global variable to store listing metadata\n",
    "listing_metadata = []\n",
    "\n",
    "def generate_image_path(listing_id: str) -> str:\n",
    "    \"\"\"Generate a path for a listing image.\"\"\"\n",
    "    images_dir = os.path.join(CHROMA_PATH, \"images\")\n",
    "    if not os.path.exists(images_dir):\n",
    "        os.makedirs(images_dir)\n",
    "    return os.path.join(images_dir, f\"{listing_id}.jpg\")\n",
    "\n",
    "def save_listings_to_txt(listings, filename=\"Listings.txt\"):\n",
    "    \"\"\"\n",
    "    Save the listings to a .txt file in a readable format.\n",
    "\n",
    "    Args:\n",
    "        listings: List of dictionaries containing listing information.\n",
    "        filename: Output .txt file name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            for idx, listing in enumerate(listings, start=1):\n",
    "                file.write(f\"Listing {idx}:\\n\")\n",
    "                for key, value in listing.items():\n",
    "                    file.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "                file.write(\"\\n\" + \"-\"*40 + \"\\n\\n\")\n",
    "        print(f\"Saved {len(listings)} listings to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to file: {e}\")\n",
    "\n",
    "def generate_real_estate_listings(n: int = 10):\n",
    "    \"\"\"\n",
    "    Generate diverse and realistic real estate listings using LLM.\n",
    "    \n",
    "    Args:\n",
    "        n: Number of listings to generate\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing listing information\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate {n} diverse and realistic real estate listings. \n",
    "    Each listing should include:\n",
    "    - Title\n",
    "    - Description\n",
    "    - Price\n",
    "    - Location\n",
    "    - Number of bedrooms\n",
    "    - Number of bathrooms\n",
    "    - Square footage\n",
    "    - Amenities\n",
    "    - Neighborhood Description\n",
    "\n",
    "    Format the output as a JSON list with these exact keys:\n",
    "    \"title\", \"description\", \"price\", \"location\", \"bedrooms\", \n",
    "    \"bathrooms\", \"square_footage\", \"amenities\", \"neighborhood\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        json_text = response.content\n",
    "        listings = json.loads(json_text)\n",
    "        save_listings_to_txt(listings)           \n",
    "        return listings\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating listings: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_listing_image(listing: Dict):\n",
    "    \"\"\"\n",
    "    Generate a placeholder image for a listing based on its features.\n",
    "    \"\"\"\n",
    "    # Create a simple placeholder image with text\n",
    "    from PIL import ImageDraw, ImageFont\n",
    "    img = Image.new('RGB', (300, 200), color=(random.randint(200, 255), \n",
    "                                            random.randint(200, 255), \n",
    "                                            random.randint(200, 255)))\n",
    "    d = ImageDraw.Draw(img)\n",
    "    \n",
    "    # Use a default font (size might need adjustment based on your system)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 15)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    d.text((10,10), f\"{listing['title']}\\n{listing['location']}\\n${listing['price']}\", \n",
    "           fill=(0,0,0), font=font)\n",
    "    return img\n",
    "\n",
    "def store_listings(listings: List[Dict]) -> None:\n",
    "    \"\"\"\n",
    "    Store listings in ChromaDB with text and image embeddings.\n",
    "    \n",
    "    Args:\n",
    "        listings: List of listing dictionaries\n",
    "    \"\"\"\n",
    "    global listing_metadata\n",
    "    \n",
    "    # Clear existing data by getting all IDs first, then deleting them\n",
    "    try:\n",
    "        # Get all existing IDs\n",
    "        existing_items = collection.get()\n",
    "        if existing_items['ids']:\n",
    "            collection.delete(ids=existing_items['ids'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error clearing existing data: {e}\")\n",
    "    \n",
    "    listing_metadata = []\n",
    "    \n",
    "    for listing in listings:\n",
    "        try:\n",
    "            # Generate unique ID for the listing\n",
    "            listing_id = str(uuid.uuid4())\n",
    "            \n",
    "            # Generate and save image for the listing\n",
    "            img = generate_listing_image(listing)\n",
    "            img_path = generate_image_path(listing_id)\n",
    "            img.save(img_path)\n",
    "            \n",
    "            # Create combined text for embedding\n",
    "            combined_text = (\n",
    "                f\"{listing['title']} {listing['description']} \"\n",
    "                f\"{listing['location']} {listing['amenities']} \"\n",
    "                f\"{listing['neighborhood']}\"\n",
    "            )\n",
    "            \n",
    "            # Generate text embedding\n",
    "            text_embedding = text_model.encode(combined_text).tolist()\n",
    "            \n",
    "            # Generate image embedding\n",
    "            inputs = clip_processor(images=img, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.get_image_features(**inputs)\n",
    "            image_embedding = image_features.squeeze().tolist()\n",
    "            \n",
    "            # Combine text and image embeddings (simple average)\n",
    "            combined_embedding = [\n",
    "                (t + i)/2 for t, i in zip(text_embedding, image_embedding)\n",
    "            ]\n",
    "            \n",
    "            # Store in ChromaDB\n",
    "            collection.add(\n",
    "                ids=[listing_id],\n",
    "                embeddings=[combined_embedding],\n",
    "                documents=[combined_text],\n",
    "                metadatas=[{\n",
    "                    \"type\": \"real_estate\",\n",
    "                    \"source\": \"generated\"\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            # Store metadata for reference\n",
    "            listing_metadata.append({\n",
    "                \"id\": listing_id,\n",
    "                \"title\": listing[\"title\"],\n",
    "                \"description\": listing[\"description\"],\n",
    "                \"price\": listing[\"price\"],\n",
    "                \"location\": listing[\"location\"],\n",
    "                \"bedrooms\": listing[\"bedrooms\"],\n",
    "                \"bathrooms\": listing[\"bathrooms\"],\n",
    "                \"square_footage\": listing[\"square_footage\"],\n",
    "                \"amenities\": listing[\"amenities\"],\n",
    "                \"neighborhood\": listing[\"neighborhood\"],\n",
    "                \"image_path\": img_path\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error storing listing {listing['title']}: {e}\")\n",
    "            continue\n",
    "        \n",
    "def compute_combined_embedding(text: str, image: Optional[Image.Image]):\n",
    "    \"\"\"\n",
    "    Compute combined embedding from text and optional image.\n",
    "    \n",
    "    Args:\n",
    "        text: Search query text\n",
    "        image: Optional PIL Image\n",
    "        \n",
    "    Returns:\n",
    "        Combined embedding vector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Compute text embedding\n",
    "        text_embedding = text_model.encode(text).tolist()\n",
    "        \n",
    "        if image is not None:\n",
    "            # Compute image embedding\n",
    "            inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.get_image_features(**inputs)\n",
    "            image_embedding = image_features.squeeze().tolist()\n",
    "            \n",
    "            # Combine embeddings (weighted average - you can adjust weights)\n",
    "            combined_embedding = [\n",
    "                0.7 * t + 0.3 * i for t, i in zip(text_embedding, image_embedding)\n",
    "            ]\n",
    "        else:\n",
    "            combined_embedding = text_embedding\n",
    "            \n",
    "        return combined_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing embeddings: {e}\")\n",
    "        return text_model.encode(\"\").tolist()  # Return empty embedding on error\n",
    "\n",
    "def augment_description(listing: Dict, preferences: str) -> str:\n",
    "    \"\"\"\n",
    "    Augment listing description based on buyer preferences using LLM.\n",
    "    \n",
    "    Args:\n",
    "        listing: The listing dictionary\n",
    "        preferences: Buyer's preferences text\n",
    "        \n",
    "    Returns:\n",
    "        Augmented description string\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a real estate agent personalizing a property description for a buyer.\n",
    "    \n",
    "    Original listing:\n",
    "    Title: {listing['title']}\n",
    "    Location: {listing['location']}\n",
    "    Price: {listing['price']}\n",
    "    Bedrooms: {listing['bedrooms']}\n",
    "    Bathrooms: {listing['bathrooms']}\n",
    "    Square footage: {listing['square_footage']}\n",
    "    Amenities: {listing['amenities']}\n",
    "    Neighborhood: {listing['neighborhood']}\n",
    "    Description: {listing['description']}\n",
    "    \n",
    "    Buyer preferences: {preferences}\n",
    "    \n",
    "    Create a personalized description that:\n",
    "    1. Maintains all factual information from the original listing\n",
    "    2. Highlights aspects that match the buyer's preferences\n",
    "    3. Is engaging and appealing to the buyer\n",
    "    4. Is 2-3 paragraphs long\n",
    "    \n",
    "    Personalized description:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error augmenting description: {e}\")\n",
    "        return listing['description']\n",
    "\n",
    "def search_listings(query: str, image: Optional[Image.Image]):\n",
    "    \"\"\"\n",
    "    Search listings based on text and optional image query.\n",
    "    \n",
    "    Args:\n",
    "        query: Text search query\n",
    "        image: Optional image search query\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (results text, list of result images)\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        combined_query_embedding = compute_combined_embedding(query, image)\n",
    "        results = collection.query(\n",
    "            query_embeddings=[combined_query_embedding],\n",
    "            n_results=3\n",
    "        )\n",
    "        \n",
    "        if not results[\"ids\"]:\n",
    "            return \"No results found.\", None\n",
    "            \n",
    "        # Get matching listings from metadata\n",
    "        matched_listings = []\n",
    "        matched_images = []\n",
    "        result_strs = []\n",
    "        \n",
    "        for listing_id in results[\"ids\"][0]:\n",
    "            listing = next((l for l in listing_metadata if l[\"id\"] == listing_id), None)\n",
    "            if listing:\n",
    "                matched_listings.append(listing)\n",
    "                matched_images.append(Image.open(listing[\"image_path\"]))\n",
    "                \n",
    "                # Augment description with buyer preferences\n",
    "                augmented_desc = augment_description(listing, query)\n",
    "                \n",
    "                # Format price as string to avoid formatting issues\n",
    "                try:\n",
    "                    price_str = \"${:,.0f}\".format(float(listing['price']))\n",
    "                except (ValueError, TypeError):\n",
    "                    price_str = str(listing['price'])\n",
    "                \n",
    "                result_strs.append(\n",
    "                    f\"üè† **{listing['title']}**\\n\"\n",
    "                    f\"üìç {listing['location']}\\n\"\n",
    "                    f\"üí∞ {price_str}\\n\"\n",
    "                    f\"üõèÔ∏è {listing['bedrooms']} bed | üõÅ {listing['bathrooms']} bath | üìè {listing['square_footage']} sqft\\n\"\n",
    "                    f\"\\n**Personalized Description:**\\n{augmented_desc}\\n\"\n",
    "                    f\"\\n**Amenities:** {listing['amenities']}\\n\"\n",
    "                    f\"\\n**Neighborhood:** {listing['neighborhood']}\"\n",
    "                )\n",
    "        \n",
    "        return \"\\n\\n\".join(result_strs), matched_images\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching listings: {e}\")\n",
    "        return \"An error occurred during search.\", None\n",
    "\n",
    "def initialize_database() -> None:\n",
    "    \"\"\"Initialize the database with generated listings and save to file.\"\"\"\n",
    "    print(\"Generating listings...\")\n",
    "    listings = generate_real_estate_listings(10)\n",
    "    if listings:\n",
    "        print(f\"Generated {len(listings)} listings\")\n",
    "        # Store in database\n",
    "        print(\"Storing listings...\")\n",
    "        store_listings(listings)\n",
    "        print(\"Database initialized successfully\")\n",
    "    else:\n",
    "        print(\"Failed to generate listings\")\n",
    "\n",
    "\n",
    "initialize_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio UI\n",
    "with gr.Blocks(title=\"Real Estate Search\") as demo:\n",
    "    gr.Markdown(\"\"\"# üè° Real Estate Multimodal Search Engine\n",
    "    Upload listings and search using a description and optional image.\"\"\")\n",
    "    \n",
    "    with gr.Tab(\"Search Listings\"):\n",
    "        with gr.Row():\n",
    "            query_input = gr.Textbox(\n",
    "                label=\"Buyer Preferences\", \n",
    "                placeholder=\"e.g., Modern house with a pool and garden near schools.\",\n",
    "                lines=3\n",
    "            )\n",
    "            query_img_input = gr.Image(\n",
    "                type=\"pil\", \n",
    "                label=\"Upload Preference Image (Optional)\"\n",
    "            )\n",
    "        search_btn = gr.Button(\"Search\", variant=\"primary\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            result_text = gr.Textbox(\n",
    "                label=\"Search Results\", \n",
    "                interactive=False,\n",
    "                lines=10,\n",
    "                max_lines=20\n",
    "            )\n",
    "            result_gallery = gr.Gallery(\n",
    "                label=\"Matching Listings\",\n",
    "                object_fit=\"contain\",\n",
    "                height=\"auto\"\n",
    "            )\n",
    "        \n",
    "        search_btn.click(\n",
    "            fn=search_listings,\n",
    "            inputs=[query_input, query_img_input],\n",
    "            outputs=[result_text, result_gallery]\n",
    "        )\n",
    "    \n",
    "    with gr.Tab(\"Database Info\"):\n",
    "        gr.Markdown(\"### Current Database Status\")\n",
    "        db_status = gr.Textbox(label=\"Listings Count\", value=f\"{len(listing_metadata)} listings\")\n",
    "        refresh_btn = gr.Button(\"Refresh Count\")\n",
    "        \n",
    "        def update_db_status():\n",
    "            return f\"{len(listing_metadata)} listings\"\n",
    "            \n",
    "        refresh_btn.click(\n",
    "            fn=update_db_status,\n",
    "            inputs=None,\n",
    "            outputs=db_status\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
